{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c878d11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset.dataset_configures'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimport_ipynb\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_configures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     edge_idx_map,\n\u001b[0;32m      7\u001b[0m     node_idx_map,\n\u001b[0;32m      8\u001b[0m     edge_adj_mat,\n\u001b[0;32m      9\u001b[0m     edge_degree_list,\n\u001b[0;32m     10\u001b[0m     edge_spd\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataset.dataset_configures'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "from t_graphormer.dataset.dataset_configures import (\n",
    "    edge_idx_map,\n",
    "    node_idx_map,\n",
    "    edge_adj_mat,\n",
    "    edge_degree_list,\n",
    "    edge_spd\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12e9e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r'C:\\Users\\gauoo\\OneDrive\\문서\\GitHub\\Traffic_Congestion_Prediction\\Traffic-Congestion-Prediction\\t-graphormer'\n",
    "sys.path.remove(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e738e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauoo\\OneDrive\\문서\\GitHub\\Traffic_Congestion_Prediction\\Traffic-Congestion-Prediction\n",
      "c:\\Users\\gauoo\\miniconda3\\envs\\d2l\\python39.zip\n",
      "c:\\Users\\gauoo\\miniconda3\\envs\\d2l\\DLLs\n",
      "c:\\Users\\gauoo\\miniconda3\\envs\\d2l\\lib\n",
      "c:\\Users\\gauoo\\miniconda3\\envs\\d2l\n",
      "\n",
      "c:\\Users\\gauoo\\miniconda3\\envs\\d2l\\lib\\site-packages\n",
      "c:\\Users\\gauoo\\miniconda3\\envs\\d2l\\lib\\site-packages\\win32\n",
      "c:\\Users\\gauoo\\miniconda3\\envs\\d2l\\lib\\site-packages\\win32\\lib\n",
      "c:\\Users\\gauoo\\miniconda3\\envs\\d2l\\lib\\site-packages\\Pythonwin\n"
     ]
    }
   ],
   "source": [
    "for p in sys.path:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "from ..dataset.data_configures import (\n",
    "    edge_idx_map,\n",
    "    node_idx_map,\n",
    "    edge_adj_mat,\n",
    "    edge_degree_list,\n",
    "    edge_spd\n",
    ")\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    \"\"\"\n",
    "    트래픽 시계열 데이터를 엣지별 시퀀스 + 시간 특성(요일, 시각)과 함께 반환합니다.\n",
    "    입력: traffic_data (T_total, E, C_all)\n",
    "    출력: past_edges, future_edges 등\n",
    "    \"\"\"\n",
    "    def __init__(self, traffic_data, window=12, week_steps=480*7):\n",
    "        super().__init__()\n",
    "        self.traffic = traffic_data          # (T_total, E, C_all)\n",
    "        self.window = window                # 과거 시퀀스 길이(12)\n",
    "        self.week_steps = week_steps\n",
    "        self.day_steps = week_steps // 7     # 하루당 스텝 수 = 480 (8시간 * 60분)\n",
    "        self.E = traffic_data.shape[1]       # 엣지 개수\n",
    "\n",
    "        T_total = traffic_data.shape[0]\n",
    "        # 과거 window를 사용하므로 시작 인덱스 범위 재계산\n",
    "        self.min_start = (T_total // week_steps - 1) * week_steps + (self.window - 1)\n",
    "        # +12까지 참조하므로 max_start = T_total - max(future offsets) - 1\n",
    "        self.max_start = T_total - 12 - 1\n",
    "        self.starts = list(range(self.min_start, self.max_start + 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.starts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t0 = self.starts[idx]\n",
    "        # 1) 과거 window 길이만큼\n",
    "        past_idxs = np.arange(t0 - self.window + 1, t0 + 1)                  # shape=(12,)\n",
    "        # 2) 미래 3개 시점: +3, +6, +12\n",
    "        fut_offsets = np.array([3, 6, 12], dtype=np.int64)\n",
    "        fut_idxs = t0 + fut_offsets                                         # shape=(3,)\n",
    "\n",
    "        # raw slice\n",
    "        past = self.traffic[past_idxs]                                       # (12, E, C_all)\n",
    "        fut  = self.traffic[fut_idxs]                                        # (3,  E, C_all)\n",
    "\n",
    "        # 채널 0~2: volume, density, flow\n",
    "        Xp = torch.from_numpy(past[..., :3]).float()                        # (12, E, 3)\n",
    "        Xf = torch.from_numpy(fut[...,  :3]).float()                        # (3,  E, 3)\n",
    "\n",
    "        # 시간 특성\n",
    "        tod_enc = ((past_idxs % self.day_steps) * 24.0 / self.day_steps).astype(np.float32)\n",
    "        dow_enc = ((past_idxs // self.day_steps) % 7).astype(np.int64)\n",
    "        tod_dec = (((fut_idxs) % self.day_steps) * 24.0 / self.day_steps).astype(np.float32)\n",
    "        dow_dec = (((fut_idxs) // self.day_steps) % 7).astype(np.int64)\n",
    "\n",
    "        # (seq, E, 1)\n",
    "        tod_feat_enc = torch.from_numpy(tod_enc)[:, None].expand(-1, self.E).unsqueeze(-1)\n",
    "        dow_feat_enc = torch.from_numpy(dow_enc).float()[:, None].expand(-1, self.E).unsqueeze(-1)\n",
    "        tod_feat_dec = torch.from_numpy(tod_dec)[:, None].expand(-1, self.E).unsqueeze(-1)\n",
    "        dow_feat_dec = torch.from_numpy(dow_dec).float()[:, None].expand(-1, self.E).unsqueeze(-1)\n",
    "\n",
    "        # 최종 엣지 피처\n",
    "        past_edges   = torch.cat([Xp, tod_feat_enc, dow_feat_enc], dim=-1)   # (12, E, 5)\n",
    "        future_edges = torch.cat([Xf, tod_feat_dec, dow_feat_dec], dim=-1)   # (3,  E, 3)\n",
    "        future_edges = future_edges[:,:,:3]\n",
    "\n",
    "        # 장기 히스토리 (5주치)\n",
    "        K = self.traffic.shape[0] // self.week_steps\n",
    "        hist_long = torch.zeros((self.window, self.E, K, 3), dtype=torch.float32)\n",
    "        for tau in range(1, self.window + 1):\n",
    "            for k in range(1, K + 1):\n",
    "                t_hist = t0 + tau - k * self.week_steps\n",
    "                frame = self.traffic[t_hist]\n",
    "                hist_long[tau-1, :, k-1] = torch.from_numpy(frame[:, :3])\n",
    "\n",
    "        return {\n",
    "            'past_edges':   past_edges,    # (12, E, 5) # 0 volume, 1 density, 2 flow, 3 tod, 4 dow\n",
    "            'future_edges': future_edges,  # (3,  E, 3)\n",
    "            'hist_long':    hist_long,     # (12, E, K, 3)\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[d2l]",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
