{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9dbc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d73fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import import_ipynb\n",
    "\n",
    "import torch\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.utils.data import random_split\n",
    "from data.traffic_dataset import TrafficDataset\n",
    "\n",
    "from src.data.wrapper import wrap_traffic_dataset\n",
    "from src.data.utils import (\n",
    "    generate_regression_task,\n",
    "    generate_split, StandardScaler\n",
    ")\n",
    "\n",
    "\n",
    "def get_connectivity(adj_matrix_path):\n",
    "    adj = sp.load_npz(adj_matrix_path)\n",
    "    edge_indices, edge_values = dense_to_sparse(torch.tensor(adj.toarray()))\n",
    "    edge_values = 1 / edge_values  # edge weights are [0, 1], convert to float\n",
    "    return edge_indices, edge_values\n",
    "\n",
    "\n",
    "def get_raw_data(dataset_path, split_ratio, n_hist, n_pred, norm):\n",
    "    assert norm, 'Traffic data should be normalized for better performance'\n",
    "\n",
    "    X_s, y_s = list(), list()\n",
    "    scaler = None\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        data_path = list(Path(dataset_path).glob(f'{split}*_hist{n_hist}_pred{n_pred}.npz'))\n",
    "\n",
    "        if data_path:\n",
    "            data = np.load(data_path[0])\n",
    "            X_s.append(data['x'])\n",
    "            y_s.append(data['y'])\n",
    "            if split == 'train':\n",
    "                scaler = StandardScaler(mean=data['mean'], std=data['std'])\n",
    "        else:\n",
    "            print(f\"preprocessed data not found at {dataset_path}, generating new data\")\n",
    "            h5_path = list(Path(dataset_path).glob('*.h5'))\n",
    "            add_time_in_day, add_time_in_week = None, None\n",
    "            if h5_path:\n",
    "                print(f'Loading data from {h5_path[0]}')\n",
    "                df = pd.read_hdf(h5_path[0])\n",
    "                add_time_in_day, add_time_in_week = True, False\n",
    "                features, targets = generate_regression_task(\n",
    "                    df, n_hist, n_pred,\n",
    "                    add_time_in_day=add_time_in_day,\n",
    "                    add_day_in_week=add_time_in_week,\n",
    "                )\n",
    "                features_fill, targets_fill = generate_regression_task(\n",
    "                    df, n_hist, n_pred,\n",
    "                    add_time_in_day=add_time_in_day,\n",
    "                    add_day_in_week=add_time_in_week,\n",
    "                    replace_drops=True,\n",
    "                )\n",
    "\n",
    "                (\n",
    "                    (train_x, val_x, test_x,\n",
    "                     train_y, val_y, test_y, scaler),\n",
    "                    train_idx, val_idx, test_idx,\n",
    "                ) = generate_split(\n",
    "                    (features, features_fill),\n",
    "                    (targets, targets_fill),\n",
    "                    split_ratio,\n",
    "                    norm\n",
    "                )\n",
    "            else:\n",
    "                data_csv_path = os.path.join(dataset_path, 'vel.csv')\n",
    "                print(f'Loading data from {data_csv_path}')\n",
    "                # process X and get node features\n",
    "                X = pd.read_csv(data_csv_path).to_numpy()  # shape [time slices, nodes]\n",
    "                if len(X.shape) == 2:\n",
    "                    X = np.expand_dims(X, 1)\n",
    "                X = X.transpose((0, 2, 1)).astype(np.float32)\n",
    "                features, targets = generate_regression_task(\n",
    "                    X, n_hist, n_pred\n",
    "                )\n",
    "                features_fill, targets_fill = generate_regression_task(\n",
    "                    X, n_hist, n_pred, replace_drops=True\n",
    "                )\n",
    "\n",
    "                (\n",
    "                    (train_x, val_x, test_x,\n",
    "                     train_y, val_y, test_y, scaler),\n",
    "                    train_idx, val_idx, test_idx,\n",
    "                ) = generate_split(\n",
    "                    (features, features_fill),\n",
    "                    (targets, targets_fill),\n",
    "                    split_ratio,\n",
    "                    norm\n",
    "                )\n",
    "\n",
    "            suffix = ''\n",
    "            if add_time_in_day is not None:\n",
    "                if add_time_in_day:\n",
    "                    suffix += '_day'\n",
    "                if add_time_in_week:\n",
    "                    suffix += '_week'\n",
    "            suffix += f'_hist{n_hist}_pred{n_pred}'\n",
    "            np.savez_compressed(\n",
    "                os.path.join(dataset_path, f'train{suffix}.npz'),\n",
    "                x=train_x,\n",
    "                y=train_y,\n",
    "                idx=train_idx,\n",
    "                mean=scaler.mean,\n",
    "                std=scaler.std\n",
    "            )\n",
    "            np.savez_compressed(\n",
    "                os.path.join(dataset_path, f'val{suffix}.npz'),\n",
    "                x=val_x,\n",
    "                y=val_y,\n",
    "                idx=val_idx\n",
    "            )\n",
    "            np.savez_compressed(\n",
    "                os.path.join(dataset_path, f'test{suffix}.npz'),\n",
    "                x=test_x,\n",
    "                y=test_y,\n",
    "                idx=test_idx\n",
    "            )\n",
    "            return train_x, val_x, test_x, train_y, val_y, test_y, scaler\n",
    "\n",
    "    return X_s + y_s + [scaler]\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "        mode: str = 'pretrain',\n",
    "        data_dir: str = None,\n",
    "        dataset_name: str = 'traffic',  # 바꿔주세요\n",
    "        n_hist: int = 12,\n",
    "        n_pred: int = 3,\n",
    "        split_ratio=(0.7, 0.2, 0.1),\n",
    "        graph_token=True,\n",
    "        seed=0,\n",
    "        task='pred',\n",
    "        norm=True,\n",
    "):\n",
    "    assert mode in [\"pretrain\", \"finetune\", \"valid\", \"test\", \"debug\"]\n",
    "    assert task == 'pred'\n",
    "\n",
    "    # 1) 데이터 불러오기\n",
    "    if not data_dir:\n",
    "        raise ValueError(\"data_dir must point to your .npy file\")\n",
    "    traffic_data = np.load(data_dir)  # e.g. 'dataset/traffic_dataset_13.npy'\n",
    "\n",
    "    # 2) TrafficDataset 생성\n",
    "    full_ds = TrafficDataset(traffic_data, window=n_hist, week_steps=480*7)\n",
    "\n",
    "    # 3) train/val/test 분할\n",
    "    total = len(full_ds)\n",
    "    n_train = int(total * split_ratio[0])\n",
    "    n_val   = int(total * split_ratio[1])\n",
    "    n_test  = total - n_train - n_val\n",
    "    train_ds, val_ds, test_ds = random_split(full_ds, [n_train, n_val, n_test])\n",
    "\n",
    "    # 4) 그래프 인코딩 정보: edge_adj_mat, edge_degree, edge_spd\n",
    "    #    이 변수들은 모듈 전역 또는 TrafficDataset 내부에서 접근 가능하도록 저장해두세요\n",
    "    from traffic_dataset import edge_adj_mat, edge_degree_list, edge_spd  # 미리 계산해 두었다고 가정\n",
    "\n",
    "    # 각 데이터셋에 이 그래프 메타정보를 붙이려면\n",
    "    for ds in (train_ds, val_ds, test_ds):\n",
    "        ds.edge_adj_mat     = edge_adj_mat\n",
    "        ds.edge_degree      = edge_degree_list\n",
    "        ds.edge_spd         = edge_spd\n",
    "\n",
    "    # 5) 반환\n",
    "    return {\n",
    "        'train_dataset': train_ds,\n",
    "        'valid_dataset': val_ds,\n",
    "        'test_dataset': test_ds,\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[d2l]",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
