{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97695f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) 필수 라이브러리 설치/임포트\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from dataset.traffic_dataset import TrafficDataset\n",
    "from dataset.dataset_config import edge_index, edge_attr\n",
    "#from models.baselines import STGCN  # 튜닝 대상 모델\n",
    "#from models.FreTSformer import FreTSformer\n",
    "from models.STLinear import STLinear\n",
    "from models.STLinear_deriven import STLinear_SPE\n",
    "from utils.Trainer import Trainer    # 앞서 만든 Trainer\n",
    "import optuna\n",
    "\n",
    "\n",
    "# 2) collate_fn 정의 (기존과 동일)\n",
    "def collate_fn(batch_list):\n",
    "    xs = torch.stack([data.x for data in batch_list], dim=0)  # [B, T, E, C]\n",
    "    ys = torch.stack([data.y for data in batch_list], dim=0)  # [B, n_pred, E, D]\n",
    "    return xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d59f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) 데이터 준비 \n",
    "dataset_np = np.load('dataset/traffic_dataset_13_smoothen.npy', allow_pickle=True)\n",
    "dataset = TrafficDataset(dataset_np, window=12, randomize=False)\n",
    "\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_size   = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=512, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 배치 한 번 꺼내서 형상 확인\n",
    "x0, y0 = next(iter(train_loader))\n",
    "B, T, E, C_in = x0.shape\n",
    "_, n_pred, _, C_out = y0.shape\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Dataset shapes: x0={x0.shape}, y0={y0.shape}, device={device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82158ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Optuna Objective 정의\n",
    "def objective(trial):\n",
    "    # --- Hyperparameter suggestions ---\n",
    "    # 공통\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
    "    dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n",
    "    # GNN 전용\n",
    "    kernel_size = trial.suggest_categorical(\"kernel_size\", [17, 33, 65])\n",
    "    K = trial.suggest_int(\"K\", 1, 3)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 4)\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 1,2,4)\n",
    "\n",
    "    input_embedding_dim = trial.suggest_categorical(\"input_embedding_dim\", [16, 32, 64])\n",
    "    tod_embedding_dim = trial.suggest_categorical(\"tod_embedding_dim\", [16, 32, 64])\n",
    "    dow_embedding_dim = trial.suggest_categorical(\"dow_embedding_dim\", [16, 32, 64])\n",
    "    spatial_embedding_dim = trial.suggest_categorical(\"spatial_embedding_dim\", [0, 16, 32, 64])\n",
    "    adaptive_embedding_dim = trial.suggest_categorical(\"adaptive_embedding_dim\", [0, 16, 32, 64])\n",
    "    spe_dim = trial.suggest_categorical(\"spe_dim\", [16, 32, 64])\n",
    "    spe_out_dim = trial.suggest_categorical(\"spe_out_dim\", [16, 32, 64])\n",
    "\n",
    "\n",
    "    model = STLinear_SPE(\n",
    "        num_nodes =E,\n",
    "        kernel_size=kernel_size, #odd number\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        input_embedding_dim = input_embedding_dim,\n",
    "        tod_embedding_dim = tod_embedding_dim,\n",
    "        dow_embedding_dim = dow_embedding_dim,\n",
    "        spatial_embedding_dim = spatial_embedding_dim,\n",
    "        adaptive_embedding_dim = adaptive_embedding_dim,\n",
    "        spe_dim = spe_dim,\n",
    "        spe_out_dim = spe_out_dim\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.L1Loss()\n",
    "\n",
    "    # --- Trainer 실행 (간단히 10 epoch) ---\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        epochs=50,\n",
    "        device=device,\n",
    "        print_interval=0,    # 출력 자제\n",
    "        plot_interval=0,     # 시각화 자제\n",
    "        early_stopping_patience=4\n",
    "    )\n",
    "    trainer.fit()\n",
    "\n",
    "    # 최종 검증 손실 반환\n",
    "    valid_loss = trainer.get_best_valid_loss() # 또는 직접 기록한 best\n",
    "    return valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac651eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Optuna Study 생성 및 최적화 실행\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "\n",
    "# 6) 최적 결과 확인\n",
    "print(\"Best validation loss:\", study.best_value)\n",
    "print(\"Best hyperparameters:\")\n",
    "for k,v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f361534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) 베스트 파라미터로 재학습 & 결과 시각화 예시\n",
    "best_params = study.best_params\n",
    "best_model = STLinear_SPE(\n",
    "    num_nodes=E,\n",
    "    # GNN 전용 파라미터\n",
    "    kernel_size=best_params['kernel_size'],        # odd number\n",
    "    num_heads=best_params['num_heads'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout=best_params['dropout'],\n",
    "    # 임베딩 차원 파라미터\n",
    "    input_embedding_dim=best_params['input_embedding_dim'],\n",
    "    tod_embedding_dim=best_params['tod_embedding_dim'],\n",
    "    dow_embedding_dim=best_params['dow_embedding_dim'],\n",
    "    spatial_embedding_dim=best_params['spatial_embedding_dim'],\n",
    "    adaptive_embedding_dim=best_params['adaptive_embedding_dim'],\n",
    "    spe_dim=best_params['spe_dim'],\n",
    "    spe_out_dim=best_params['spe_out_dim']\n",
    ").to(device)\n",
    "\n",
    "best_opt = AdamW(best_model.parameters(), lr=5e-5, weight_decay=best_params['weight_decay'])\n",
    "trainer = Trainer(\n",
    "    model=best_model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=val_loader,\n",
    "    optimizer=best_opt,\n",
    "    criterion=torch.nn.L1Loss(),\n",
    "    epochs=60,\n",
    "    device=device,\n",
    "    print_interval=0,\n",
    "    plot_interval=2,\n",
    "    auto_save=True,\n",
    "    save_dir='./final_model'\n",
    ")\n",
    "trainer.fit()\n",
    "hist = trainer.get_history()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist['train_loss'], label='Train Loss')\n",
    "plt.plot(hist['valid_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d21b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "x_batch, y_batch = next(iter(train_loader))  # (B, T, E, D), (B, T_out, E, D_out)\n",
    "x_input = x_batch[0].unsqueeze(0).to(device) # B=1로 만듦\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, attention_maps = best_model(x_input, return_attn=True)  # x_input: (B, T, E, D')\n",
    "\n",
    "\n",
    "layer0 = attention_maps[3]            # 첫 번째 레이어\n",
    "B, H, T, E, _ = layer0.shape\n",
    "\n",
    "# 2) Aggregation 예시: \n",
    "#   a) 헤드 평균 → (B, T, E, E)\n",
    "#   b) 특정 시점만 뽑기 or 시점 평균 → (B, E, E)\n",
    "# 아래는 배치0, 시점0에 대해 “헤드 평균”을 취한 맵\n",
    "attn_heads_avg = layer0[0].mean(dim=0)    # → (T, E, E)\n",
    "time_idx = 0\n",
    "attn_agg = attn_heads_avg[time_idx].cpu().numpy()  # (E, E)\n",
    "\n",
    "# or, 시점 평균까지 함께 하고 싶다면:\n",
    "# attn_agg = attn_heads_avg.mean(dim=0).cpu().numpy()  # (E, E)\n",
    "\n",
    "# 3) Clustering: 계층적 군집화로 노드 순서(order) 구하기\n",
    "#    similarity 로는 각 노드 i의 “전체 주목도”(행 평균)를 사용\n",
    "sim = attn_agg.mean(axis=1)            # (E,)\n",
    "Z = linkage(sim.reshape(-1,1), method='average')\n",
    "order = leaves_list(Z)                 # 군집화된 노드 인덱스 순서\n",
    "\n",
    "# 4) 재배열된 맵\n",
    "attn_clustered = attn_agg[order][:, order]\n",
    "\n",
    "# 5) 시각화\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(attn_clustered, cmap='viridis', \n",
    "            xticklabels=order, yticklabels=order)\n",
    "plt.title(f'Layer0 Head-Avg t={time_idx} (Clustered)')\n",
    "plt.xlabel('Reordered Key Node')\n",
    "plt.ylabel('Reordered Query Node')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "# # 예시 시각화: 첫 번째 레이어, 첫 번째 배치, 첫 타임스텝, 헤드 0\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# attn = attention_maps[2][0, 0, 0].cpu().numpy()  # (E, E)\n",
    "# sns.heatmap(attn)\n",
    "# plt.title(\"Layer 1, Head 0 Attention Map at t=0\")\n",
    "# plt.show()\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "N = attn_clustered.shape[0]\n",
    "G = nx.DiGraph()\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        w = attn_clustered[i,j]\n",
    "        if w > 0.03:  # threshold\n",
    "            G.add_edge(i, j, weight=w)\n",
    "\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(G, pos, with_labels=True,\n",
    "        width=[G[u][v]['weight']*5 for u,v in G.edges()])\n",
    "\n",
    "plt.title(\"Spatial Attention Graph (thresholded)\")\n",
    "# 4) 맨 마지막에 show()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
