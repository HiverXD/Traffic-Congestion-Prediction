{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f0b811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kim Seung Woo\\miniconda3\\envs\\mlproject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Kim Seung Woo\\miniconda3\\envs\\mlproject\\lib\\site-packages\\torch_geometric\\graphgym\\config.py:19: UserWarning: Could not define global config object. Please install 'yacs' via 'pip install yacs' in order to use GraphGym\n",
      "  warnings.warn(\"Could not define global config object. Please install \"\n",
      "c:\\Users\\Kim Seung Woo\\miniconda3\\envs\\mlproject\\lib\\site-packages\\torch_geometric\\graphgym\\imports.py:14: UserWarning: Please install 'pytorch_lightning' via  'pip install pytorch_lightning' in order to use GraphGym\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' via  \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.traffic_dataset import TrafficDataset\n",
    "from utils.Trainer import Trainer\n",
    "import numpy as np\n",
    "from models.STLinear import STLinear\n",
    "from models.FreTSformer import FreTSformer\n",
    "from models.FreTS_SE import STLinear_SPE_FreTS, STLinear_HopBiased_FreTS\n",
    "from models.baselines import GCNMLP, DCRNN, STGCN, MLPBASED, STGAT\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from dataset.dataset_config import edge_index, edge_attr, week_steps, edge_spd\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)             # CPU 연산용\n",
    "torch.cuda.manual_seed_all(seed)    # GPU 연산용 (멀티 GPU 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e358335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE\n",
    "class MAPELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    안정화를 위해 true 값이 작은 경우 eps로 클램프한 후 계산합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, true: torch.Tensor) -> torch.Tensor:\n",
    "        # 분모가 너무 작아지는 것을 방지\n",
    "        denom = torch.clamp(true.abs(), min=self.eps)\n",
    "        loss = torch.abs((pred - true) / denom)\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a874276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 & 데이터셋 준비\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WINDOW = 12                   # 입력 시퀀스 길이\n",
    "N_PRED = 3                    # 예측 시퀀스 길이\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data = np.load('dataset/traffic_dataset_13_smoothen.npy')  # (T_total, E, C_all)\n",
    "# train/valid split (예: 시계열 뒤쪽 20%를 검증으로)\n",
    "\n",
    "\n",
    "split_idx = int(data.shape[0] * 0.8)\n",
    "train_data, valid_data = data[:split_idx], data[split_idx:]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xs = torch.stack([item.x for item in batch], dim=0)\n",
    "    ys = torch.stack([item.y for item in batch], dim=0)\n",
    "    return xs, ys\n",
    "\n",
    "train_ds = TrafficDataset(train_data, window=WINDOW, randomize=True)\n",
    "valid_ds = TrafficDataset(valid_data, window=WINDOW, randomize=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca76ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 파라미터 및 모델 목록 정의\n",
    "\n",
    "DEVICE       = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "WINDOW       = 12\n",
    "PRED_OFFSETS = np.array([3, 6, 12])\n",
    "EDGE_IDS     = [2]                  # 시각화할 edge index 리스트\n",
    "T_TOTAL      = data.shape[0]        # 전체 타임스텝 수\n",
    "SAVE_DIR     = \"./checkpoints\"      \n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "models = {\n",
    "#    \"GCNMLP\":    GCNMLP,\n",
    "#    \"DCRNN\":     DCRNN,\n",
    "#    \"STGCN\":     STGCN,\n",
    "#    \"MLPBASED\":  MLPBASED,\n",
    "#    \"STGAT\": STGAT,\n",
    "     \"STLinear_SPE_FreTS\": STLinear_SPE_FreTS, # lr=1e-4, layer = 4 batch 32 : 0.36\n",
    "#     \"STLinear_HopBiased_FreTS\": STLinear_HopBiased_FreTS\n",
    "#    \"FreTSformer\" : FreTSformer, # lr=1e-4, batch 8 : 0.3575\n",
    "#    \"STAEformer\" : STAEformer, # lr= 1e-4, batch = 32 0.39\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b83a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kim Seung Woo\\Documents\\GitHub\\TCP_main\\Traffic-Congestion-Prediction\\src\\utils\\Trainer.py:52: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n",
      "c:\\Users\\Kim Seung Woo\\miniconda3\\envs\\mlproject\\lib\\site-packages\\torch\\amp\\grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training STLinear_SPE_FreTS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]c:\\Users\\Kim Seung Woo\\Documents\\GitHub\\TCP_main\\Traffic-Congestion-Prediction\\src\\utils\\Trainer.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "c:\\Users\\Kim Seung Woo\\miniconda3\\envs\\mlproject\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 91\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# (4) Trainer 초기화·학습\u001b[39;00m\n\u001b[0;32m     82\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     83\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     84\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE\n\u001b[0;32m     90\u001b[0m )\n\u001b[1;32m---> 91\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# (5) 가중치 저장\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# save_path = os.path.join(SAVE_DIR, f\"{name}.pth\")\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# torch.save(model.state_dict(), save_path)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# print(f\"Saved checkpoint: {save_path}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kim Seung Woo\\Documents\\GitHub\\TCP_main\\Traffic-Congestion-Prediction\\src\\utils\\Trainer.py:107\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m--> 107\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# pred: [B, n_pred, E, D_out]\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(pred, y_batch)\n",
      "File \u001b[1;32mc:\\Users\\Kim Seung Woo\\miniconda3\\envs\\mlproject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Kim Seung Woo\\miniconda3\\envs\\mlproject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Kim Seung Woo\\Documents\\GitHub\\TCP_main\\Traffic-Congestion-Prediction\\src\\models\\FreTS_SE.py:965\u001b[0m, in \u001b[0;36mSTLinear_SPE_FreTS.forward\u001b[1;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[0;32m    963\u001b[0m x_seq \u001b[38;5;241m=\u001b[39m x_cat\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lin \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_layers_t:\n\u001b[1;32m--> 965\u001b[0m     x_seq \u001b[38;5;241m=\u001b[39m \u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, in_steps, E, model_dim)\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;66;03m# 11) 공간 블록 반복 (Self-Attention)\u001b[39;00m\n\u001b[0;32m    968\u001b[0m x_spatial \u001b[38;5;241m=\u001b[39m x_seq\n",
      "File \u001b[1;32mc:\\Users\\Kim Seung Woo\\miniconda3\\envs\\mlproject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Kim Seung Woo\\miniconda3\\envs\\mlproject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Kim Seung Woo\\Documents\\GitHub\\TCP_main\\Traffic-Congestion-Prediction\\src\\models\\FreTS_SE.py:107\u001b[0m, in \u001b[0;36mFreTS.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    103\u001b[0m bias \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# 시간 축 연산만 수행\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# N 인자에 (Num_nodes * Input_dim)을 전달\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMLP_temporal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mD_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m bias\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Final FC layer for prediction\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# x: [B, (N*D_in), T, D_embed]\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# reshape: [B, (N*D_in), T*D_embed]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kim Seung Woo\\Documents\\GitHub\\TCP_main\\Traffic-Congestion-Prediction\\src\\models\\FreTS_SE.py:69\u001b[0m, in \u001b[0;36mFreTS.MLP_temporal\u001b[1;34m(self, x, B, N_channels_total, L)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mMLP_temporal\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, B, N_channels_total, L):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# x: [B, N_channels_total, T, D_embed]\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mrfft(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mortho\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# FFT on L (time) dimension\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFreMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_channels_total\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mi2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mib2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mirfft(y, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_length, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mortho\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Kim Seung Woo\\Documents\\GitHub\\TCP_main\\Traffic-Congestion-Prediction\\src\\models\\FreTS_SE.py:87\u001b[0m, in \u001b[0;36mFreTS.FreMLP\u001b[1;34m(self, B, nd, dimension, x, r, i, rb, ib)\u001b[0m\n\u001b[0;32m     77\u001b[0m o1_imag \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([B, nd, dimension \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_size],\n\u001b[0;32m     78\u001b[0m                       device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     80\u001b[0m o1_real \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\n\u001b[0;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbijd,dd->bijd\u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m.\u001b[39mreal, r) \u001b[38;5;241m-\u001b[39m \\\n\u001b[0;32m     82\u001b[0m     torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbijd,dd->bijd\u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m.\u001b[39mimag, i) \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     83\u001b[0m     rb\n\u001b[0;32m     84\u001b[0m )\n\u001b[0;32m     86\u001b[0m o1_imag \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\n\u001b[1;32m---> 87\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbijd,dd->bijd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbijd,dd->bijd\u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m.\u001b[39mreal, i) \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     89\u001b[0m     ib\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     92\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([o1_real, o1_imag], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     93\u001b[0m y \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftshrink(y, lambd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparsity_threshold)\n",
      "File \u001b[1;32mc:\\Users\\Kim Seung Woo\\miniconda3\\envs\\mlproject\\lib\\site-packages\\torch\\functional.py:422\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    424\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "for name, ModelClass in models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    # (1) 배치 데이터에서 차원 정보 추출\n",
    "    x_sample, y_sample = next(iter(train_loader))\n",
    "    _, T, E, D_in = x_sample.shape\n",
    "    _, _, _, D_out = y_sample.shape\n",
    "    n_pred = y_sample.size(1)\n",
    "\n",
    "    # (2) 모델 인스턴스화\n",
    "    if name == \"MLPBASED\":\n",
    "        model = ModelClass(T=T, E=E, D_in=D_in, n_pred=n_pred, D_out=D_out,\n",
    "                           hidden_dim=256, dropout=0.1)\n",
    "    else:\n",
    "        # GCNMLP, DCRNN, STGCN 공통 생성자 인자\n",
    "        kwargs = dict(\n",
    "            num_nodes=E, node_feature_dim=D_in,\n",
    "            pred_node_dim=D_out, n_pred=n_pred,\n",
    "            encoder_embed_dim=256, encoder_depth=3,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        if name == \"DCRNN\":\n",
    "            kwargs.update(K=2)\n",
    "        elif name == \"STGCN\":\n",
    "            kwargs.update(kernel_size=3, K=2)\n",
    "        elif name == \"GCNMLP\":\n",
    "            kwargs.update(mlp_hidden_dim=16, mlp_pred_dropout=0.0)\n",
    "        elif name == \"STGAT\":\n",
    "            kwargs.update(encoder_depth=2, kernel_size=3, heads=4)\n",
    "        elif name == \"STLinear\":\n",
    "            kwargs = dict(num_nodes = 50)\n",
    "        elif name == \"FreTSformer\":\n",
    "            kwargs = dict(\n",
    "                num_nodes=50,\n",
    "                in_steps=12,\n",
    "                out_steps=3,\n",
    "                steps_per_day=480,\n",
    "                input_dim=3,\n",
    "                output_dim=3,\n",
    "                input_embedding_dim=24,\n",
    "                tod_embedding_dim=24,\n",
    "                dow_embedding_dim=24,\n",
    "                spatial_embedding_dim=0,\n",
    "                adaptive_embedding_dim=80,\n",
    "                feed_forward_dim=256,\n",
    "                num_heads=4,\n",
    "                num_layers=3,\n",
    "                dropout=0.1,\n",
    "                use_mixed_proj=True,)\n",
    "        elif name == \"STAEformer\":\n",
    "            kwargs = dict(num_nodes=50)\n",
    "        elif name == \"STLinear_SPE_FreTS\":\n",
    "            kwargs = dict(\n",
    "                num_nodes=50,\n",
    "                in_steps=12,\n",
    "                out_steps=3,\n",
    "                steps_per_day=480,\n",
    "                input_dim=3,\n",
    "                output_dim=3,\n",
    "                input_embedding_dim=24,\n",
    "                tod_embedding_dim=24,\n",
    "                dow_embedding_dim=24,\n",
    "                spatial_embedding_dim=0,\n",
    "                adaptive_embedding_dim=80,\n",
    "                feed_forward_dim=256,\n",
    "                num_heads=4,\n",
    "                num_layers=3,\n",
    "                dropout=0.1,\n",
    "                use_mixed_proj=True,\n",
    "            )   \n",
    "        elif name == \"STLinear_HopBiased_FreTS\":\n",
    "            kwargs = dict(\n",
    "                num_nodes=50,\n",
    "                in_steps=12,\n",
    "                out_steps=3,\n",
    "                steps_per_day=480,\n",
    "                input_dim=3,\n",
    "                output_dim=3,\n",
    "                input_embedding_dim=24,\n",
    "                tod_embedding_dim=24,\n",
    "                dow_embedding_dim=24,\n",
    "                spatial_embedding_dim=0,\n",
    "                adaptive_embedding_dim=80,\n",
    "                feed_forward_dim=256,\n",
    "                num_heads=4,\n",
    "                num_layers=3,\n",
    "                dropout=0.1,\n",
    "                use_mixed_proj=True,\n",
    "            )\n",
    "\n",
    "        model = ModelClass(**kwargs)\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # (3) 옵티마이저·손실함수 설정\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "    criterion = torch.nn.L1Loss()\n",
    "\n",
    "    # (4) Trainer 초기화·학습\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        epochs=1,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    trainer.fit()\n",
    "\n",
    "    # (5) 가중치 저장\n",
    "    # save_path = os.path.join(SAVE_DIR, f\"{name}.pth\")\n",
    "    # torch.save(model.state_dict(), save_path)\n",
    "    # print(f\"Saved checkpoint: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f3b45",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FreTSformer.__init__() got an unexpected keyword argument 'node_feature_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTGAT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     37\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mupdate(encoder_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m     model \u001b[38;5;241m=\u001b[39m ModelClass(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# (2) 가중치 로드 (임시. 나중에 수정해야 함.)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTGCN\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: FreTSformer.__init__() got an unexpected keyword argument 'node_feature_dim'"
     ]
    }
   ],
   "source": [
    "from dataset.traffic_dataset import EDGE_INDEX, EDGE_ATTR\n",
    "from dataset.dataset_config import week_steps, C_origin\n",
    "from utils.edge_prediction_visualization import visualize_predictions, add_tod_dow\n",
    "EDGE_INDEX = EDGE_INDEX.to(DEVICE)\n",
    "EDGE_ATTR = EDGE_ATTR.to(DEVICE)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────\n",
    "# 4) 저장된 체크포인트 로드 후 일괄 시각화\n",
    "# ─────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "for name, ModelClass in models.items():\n",
    "    # (1) 모델 재생성 (학습 시와 동일한 인자 사용)\n",
    "    #    -- 이하 2)~3)은 학습 루프와 동일하게 구성합니다.\n",
    "    x_sample, y_sample = next(iter(train_loader))\n",
    "    _, T, E, D_in = x_sample.shape\n",
    "    _, _, _, D_out = y_sample.shape\n",
    "    n_pred = y_sample.size(1)\n",
    "\n",
    "    if name == \"MLPBASED\":\n",
    "        model = ModelClass(T=T, E=E, D_in=D_in, n_pred=n_pred, D_out=D_out,\n",
    "                           hidden_dim=256, dropout=0.1)\n",
    "    else:\n",
    "        kwargs = dict(\n",
    "            num_nodes=E, node_feature_dim=D_in,\n",
    "            pred_node_dim=D_out, n_pred=n_pred,\n",
    "            encoder_embed_dim=32, encoder_depth=1,\n",
    "            dropout=0.0\n",
    "        )\n",
    "        if name == \"DCRNN\":\n",
    "            kwargs.update(K=2)\n",
    "        elif name == \"STGCN\":\n",
    "            kwargs.update(kernel_size=3, K=2)\n",
    "        elif name == \"GCNMLP\":\n",
    "            kwargs.update(mlp_hidden_dim=16, mlp_pred_dropout=0.0)\n",
    "        elif name == \"STGAT\":\n",
    "            kwargs.update(encoder_depth=2, kernel_size=3, heads=4)\n",
    "\n",
    "        model = ModelClass(**kwargs)\n",
    "\n",
    "    # (2) 가중치 로드 (임시. 나중에 수정해야 함.)\n",
    "    if name == \"STGCN\":\n",
    "        ckpt_path = os.path.join(SAVE_DIR, f\"{name}_h32_el1.pth\")\n",
    "    else:\n",
    "        ckpt_path = os.path.join(SAVE_DIR, f\"{name}.pth\")\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE, weights_only=True))\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    print(f\"\\n--- Visualizing {name} ---\")\n",
    "    EDGE_IDS = [2]\n",
    "    expanded_data = add_tod_dow(data, week_steps=week_steps, C_origin=C_origin)\n",
    "    visualize_predictions(model, expanded_data, EDGE_IDS, DEVICE, EDGE_INDEX, EDGE_ATTR, interval=(480*5,480*6), channel=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d3ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
